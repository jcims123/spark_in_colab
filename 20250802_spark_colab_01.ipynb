{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVZkpDaw3JnbsLapKcOxs5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcims123/spark_in_colab/blob/main/20250802_spark_colab_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7vzWGvg-O7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fee25ba-4088-4b0b-a33a-dd516c89ca7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "âœ… PySpark 3.5.0 installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# ===== CORRECTED PYSPARK SETUP FOR GOOGLE COLAB =====\n",
        "\n",
        "# Step 1: Install Java (required for Spark)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Step 2: Download Apache Spark 3.5.0 (stable version that works reliably)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Step 3: Install findspark\n",
        "!pip install findspark\n",
        "\n",
        "# Step 4: Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "# Step 5: Initialize Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "print(\"âœ… PySpark 3.5.0 installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Create optimized Spark session for interviews\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"InterviewPrep\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"âœ… Spark {spark.version} session created!\")\n",
        "print(f\"ðŸ”§ Using {spark.sparkContext.defaultParallelism} cores\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMa4MfbG__jx",
        "outputId": "12985224-4b82-4430-955f-8468df029fba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Spark 3.5.0 session created!\n",
            "ðŸ”§ Using 2 cores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¯ Essential Interview Code Patterns\n",
        "1. DataFrame Creation & Basic Operations"
      ],
      "metadata": {
        "id": "JwgZCNm6BqaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for interviews\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Create sample employee data\n",
        "employees_data = [\n",
        "    (1, \"John\", \"Engineering\", 75000, \"2020-01-15\"),\n",
        "    (2, \"Alice\", \"Marketing\", 65000, \"2019-03-20\"),\n",
        "    (3, \"Bob\", \"Engineering\", 80000, \"2021-06-10\"),\n",
        "    (4, \"Carol\", \"Sales\", 70000, \"2020-11-05\"),\n",
        "    (5, \"David\", \"Engineering\", 85000, \"2018-08-12\")\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"department\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True),\n",
        "    StructField(\"hire_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(employees_data, schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "# Common interview operations\n",
        "print(f\"Total employees: {df.count()}\")\n",
        "print(f\"Columns: {df.columns}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5joU22P_A8xR",
        "outputId": "bad38441-9800-4399-9c46-dcdd795c1583"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-----------+------+----------+\n",
            "| id| name| department|salary| hire_date|\n",
            "+---+-----+-----------+------+----------+\n",
            "|  1| John|Engineering| 75000|2020-01-15|\n",
            "|  2|Alice|  Marketing| 65000|2019-03-20|\n",
            "|  3|  Bob|Engineering| 80000|2021-06-10|\n",
            "|  4|Carol|      Sales| 70000|2020-11-05|\n",
            "|  5|David|Engineering| 85000|2018-08-12|\n",
            "+---+-----+-----------+------+----------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- hire_date: string (nullable = true)\n",
            "\n",
            "Total employees: 5\n",
            "Columns: ['id', 'name', 'department', 'salary', 'hire_date']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Transformations vs Actions (Critical Interview Topic)"
      ],
      "metadata": {
        "id": "81G5qU49Bzw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMATIONS (Lazy - not executed immediately)\n",
        "filtered_df = df.filter(col(\"salary\") > 70000)  # Lazy\n",
        "high_earners = df.select(\"name\", \"salary\").filter(col(\"salary\") > 75000)  # Lazy\n",
        "dept_grouped = df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))  # Lazy\n",
        "\n",
        "# ACTIONS (Eager - trigger execution)\n",
        "filtered_df.show()  # Action - executes the transformation\n",
        "count_result = df.count()  # Action\n",
        "collected_data = df.collect()  # Action - brings all data to driver\n",
        "\n",
        "print(\"âœ… Transformations are lazy, Actions trigger execution\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf6itw5zBQCs",
        "outputId": "f795fa48-1f61-4b85-bf55-f1d599567fdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-----------+------+----------+\n",
            "| id| name| department|salary| hire_date|\n",
            "+---+-----+-----------+------+----------+\n",
            "|  1| John|Engineering| 75000|2020-01-15|\n",
            "|  3|  Bob|Engineering| 80000|2021-06-10|\n",
            "|  5|David|Engineering| 85000|2018-08-12|\n",
            "+---+-----+-----------+------+----------+\n",
            "\n",
            "âœ… Transformations are lazy, Actions trigger execution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Advanced Joins (Very Common in Interviews)"
      ],
      "metadata": {
        "id": "1cB4rvpaB6lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create second dataset for joins\n",
        "projects_data = [\n",
        "    (101, \"Project A\", 1),\n",
        "    (102, \"Project B\", 3),\n",
        "    (103, \"Project C\", 1),\n",
        "    (104, \"Project D\", 5),  # Employee 5 exists\n",
        "    (105, \"Project E\", 6)   # Employee 6 doesn't exist\n",
        "]\n",
        "\n",
        "projects_schema = StructType([\n",
        "    StructField(\"project_id\", IntegerType(), True),\n",
        "    StructField(\"project_name\", StringType(), True),\n",
        "    StructField(\"lead_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "projects_df = spark.createDataFrame(projects_data, projects_schema)\n",
        "\n",
        "# Different join types (common interview question)\n",
        "print(\"=== INNER JOIN ===\")\n",
        "inner_join = df.join(projects_df, df.id == projects_df.lead_id, \"inner\")\n",
        "inner_join.select(\"name\", \"department\", \"project_name\").show()\n",
        "\n",
        "print(\"=== LEFT JOIN ===\")\n",
        "left_join = df.join(projects_df, df.id == projects_df.lead_id, \"left\")\n",
        "left_join.select(\"name\", \"department\", \"project_name\").show()\n",
        "\n",
        "print(\"=== RIGHT JOIN ===\")\n",
        "right_join = df.join(projects_df, df.id == projects_df.lead_id, \"right\")\n",
        "right_join.select(\"name\", \"department\", \"project_name\").show()\n",
        "\n",
        "# Broadcast joins for performance (important optimization topic)\n",
        "from pyspark.sql.functions import broadcast\n",
        "broadcast_join = df.join(broadcast(projects_df), df.id == projects_df.lead_id, \"inner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVT2UatvB1_e",
        "outputId": "b55b5ebb-429a-402e-973e-8fc6a897ab19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== INNER JOIN ===\n",
            "+-----+-----------+------------+\n",
            "| name| department|project_name|\n",
            "+-----+-----------+------------+\n",
            "| John|Engineering|   Project A|\n",
            "| John|Engineering|   Project C|\n",
            "|  Bob|Engineering|   Project B|\n",
            "|David|Engineering|   Project D|\n",
            "+-----+-----------+------------+\n",
            "\n",
            "=== LEFT JOIN ===\n",
            "+-----+-----------+------------+\n",
            "| name| department|project_name|\n",
            "+-----+-----------+------------+\n",
            "| John|Engineering|   Project C|\n",
            "| John|Engineering|   Project A|\n",
            "|Alice|  Marketing|        NULL|\n",
            "|  Bob|Engineering|   Project B|\n",
            "|David|Engineering|   Project D|\n",
            "|Carol|      Sales|        NULL|\n",
            "+-----+-----------+------------+\n",
            "\n",
            "=== RIGHT JOIN ===\n",
            "+-----+-----------+------------+\n",
            "| name| department|project_name|\n",
            "+-----+-----------+------------+\n",
            "| John|Engineering|   Project A|\n",
            "|  Bob|Engineering|   Project B|\n",
            "| John|Engineering|   Project C|\n",
            "| NULL|       NULL|   Project E|\n",
            "|David|Engineering|   Project D|\n",
            "+-----+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EO3YFb9dB8oY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}