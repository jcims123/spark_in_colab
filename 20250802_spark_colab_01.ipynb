{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL77GdiBo6Yh/NXrf5bIsP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcims123/spark_in_colab/blob/main/20250802_spark_colab_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark Technical Interview Preparation Guide for Google Colab (2025)\n",
        "ðŸš€ Quick Google Colab Setup for PySpark\n",
        "1. Complete PySpark Installation (Latest Version 3.5+)"
      ],
      "metadata": {
        "id": "PYHgDgL-DI_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7vzWGvg-O7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fee25ba-4088-4b0b-a33a-dd516c89ca7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "âœ… PySpark 3.5.0 installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# ===== CORRECTED PYSPARK SETUP FOR GOOGLE COLAB =====\n",
        "\n",
        "# Step 1: Install Java (required for Spark)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Step 2: Download Apache Spark 3.5.0 (stable version that works reliably)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Step 3: Install findspark\n",
        "!pip install findspark\n",
        "\n",
        "# Step 4: Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "# Step 5: Initialize Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "print(\"âœ… PySpark 3.5.0 installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create Optimized Spark Session"
      ],
      "metadata": {
        "id": "vYLfyCVEDQV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Create optimized Spark session for interviews\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"InterviewPrep\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"âœ… Spark {spark.version} session created!\")\n",
        "print(f\"ðŸ”§ Using {spark.sparkContext.defaultParallelism} cores\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMa4MfbG__jx",
        "outputId": "12985224-4b82-4430-955f-8468df029fba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Spark 3.5.0 session created!\n",
            "ðŸ”§ Using 2 cores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“š Latest PySpark 2025 Key Features & Updates\n",
        "Current Version: PySpark 3.5.1 (February 2025)\n",
        "Major New Features in 3.5.x:\n",
        "\n",
        "- Enhanced Pandas API on Spark (formerly Koalas)  \n",
        "- Improved Structured Streaming with better watermarking  \n",
        "- Connect - New client-server architecture  \n",
        "- Enhanced ML Pipeline optimizations.\n",
        "- Better Cloud integration (AWS, Azure, GCP).\n",
        "\n",
        "Core Interview Topics for 2025:\n",
        "\n",
        "DataFrames & Spark SQL (Most important)\n",
        "RDDs & Transformations/Actions\n",
        "Performance Optimization\n",
        "Structured Streaming\n",
        "MLlib & Machine Learning Pipelines\n",
        "Memory Management & Caching\n",
        "Joins & Window Functions"
      ],
      "metadata": {
        "id": "KDcdGvHUDTzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸŽ¯ Essential Interview Code Patterns\n",
        "1. DataFrame Creation & Basic Operations"
      ],
      "metadata": {
        "id": "JwgZCNm6BqaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for interviews\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Create sample employee data\n",
        "employees_data = [\n",
        "    (1, \"John\", \"Engineering\", 75000, \"2020-01-15\"),\n",
        "    (2, \"Alice\", \"Marketing\", 65000, \"2019-03-20\"),\n",
        "    (3, \"Bob\", \"Engineering\", 80000, \"2021-06-10\"),\n",
        "    (4, \"Carol\", \"Sales\", 70000, \"2020-11-05\"),\n",
        "    (5, \"David\", \"Engineering\", 85000, \"2018-08-12\")\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"department\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True),\n",
        "    StructField(\"hire_date\", StringType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(employees_data, schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "# Common interview operations\n",
        "print(f\"Total employees: {df.count()}\")\n",
        "print(f\"Columns: {df.columns}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5joU22P_A8xR",
        "outputId": "bad38441-9800-4399-9c46-dcdd795c1583"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-----------+------+----------+\n",
            "| id| name| department|salary| hire_date|\n",
            "+---+-----+-----------+------+----------+\n",
            "|  1| John|Engineering| 75000|2020-01-15|\n",
            "|  2|Alice|  Marketing| 65000|2019-03-20|\n",
            "|  3|  Bob|Engineering| 80000|2021-06-10|\n",
            "|  4|Carol|      Sales| 70000|2020-11-05|\n",
            "|  5|David|Engineering| 85000|2018-08-12|\n",
            "+---+-----+-----------+------+----------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- hire_date: string (nullable = true)\n",
            "\n",
            "Total employees: 5\n",
            "Columns: ['id', 'name', 'department', 'salary', 'hire_date']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Transformations vs Actions (Critical Interview Topic)"
      ],
      "metadata": {
        "id": "81G5qU49Bzw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFORMATIONS (Lazy - not executed immediately)\n",
        "filtered_df = df.filter(col(\"salary\") > 70000)  # Lazy\n",
        "high_earners = df.select(\"name\", \"salary\").filter(col(\"salary\") > 75000)  # Lazy\n",
        "dept_grouped = df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))  # Lazy\n",
        "\n",
        "# ACTIONS (Eager - trigger execution)\n",
        "filtered_df.show()  # Action - executes the transformation\n",
        "count_result = df.count()  # Action\n",
        "collected_data = df.collect()  # Action - brings all data to driver\n",
        "\n",
        "print(\"âœ… Transformations are lazy, Actions trigger execution\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf6itw5zBQCs",
        "outputId": "f795fa48-1f61-4b85-bf55-f1d599567fdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-----------+------+----------+\n",
            "| id| name| department|salary| hire_date|\n",
            "+---+-----+-----------+------+----------+\n",
            "|  1| John|Engineering| 75000|2020-01-15|\n",
            "|  3|  Bob|Engineering| 80000|2021-06-10|\n",
            "|  5|David|Engineering| 85000|2018-08-12|\n",
            "+---+-----+-----------+------+----------+\n",
            "\n",
            "âœ… Transformations are lazy, Actions trigger execution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Advanced Joins (Very Common in Interviews)"
      ],
      "metadata": {
        "id": "1cB4rvpaB6lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create second dataset for joins\n",
        "projects_data = [\n",
        "    (101, \"Project A\", 1),\n",
        "    (102, \"Project B\", 3),\n",
        "    (103, \"Project C\", 1),\n",
        "    (104, \"Project D\", 5),  # Employee 5 exists\n",
        "    (105, \"Project E\", 6)   # Employee 6 doesn't exist\n",
        "]\n",
        "\n",
        "projects_schema = StructType([\n",
        "    StructField(\"project_id\", IntegerType(), True),\n",
        "    StructField(\"project_name\", StringType(), True),\n",
        "    StructField(\"lead_id\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "projects_df = spark.createDataFrame(projects_data, projects_schema)\n",
        "\n",
        "# Different join types (common interview question)\n",
        "print(\"=== INNER JOIN ===\")\n",
        "inner_join = df.join(projects_df, df.id == projects_df.lead_id, \"inner\")\n",
        "inner_join.select(\"name\", \"department\", \"project_name\").show()\n",
        "\n",
        "print(\"=== LEFT JOIN ===\")\n",
        "left_join = df.join(projects_df, df.id == projects_df.lead_id, \"left\")\n",
        "left_join.select(\"name\", \"department\", \"project_name\").show()\n",
        "\n",
        "print(\"=== RIGHT JOIN ===\")\n",
        "right_join = df.join(projects_df, df.id == projects_df.lead_id, \"right\")\n",
        "right_join.select(\"name\", \"department\", \"project_name\").show()\n",
        "\n",
        "# Broadcast joins for performance (important optimization topic)\n",
        "from pyspark.sql.functions import broadcast\n",
        "broadcast_join = df.join(broadcast(projects_df), df.id == projects_df.lead_id, \"inner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVT2UatvB1_e",
        "outputId": "b55b5ebb-429a-402e-973e-8fc6a897ab19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== INNER JOIN ===\n",
            "+-----+-----------+------------+\n",
            "| name| department|project_name|\n",
            "+-----+-----------+------------+\n",
            "| John|Engineering|   Project A|\n",
            "| John|Engineering|   Project C|\n",
            "|  Bob|Engineering|   Project B|\n",
            "|David|Engineering|   Project D|\n",
            "+-----+-----------+------------+\n",
            "\n",
            "=== LEFT JOIN ===\n",
            "+-----+-----------+------------+\n",
            "| name| department|project_name|\n",
            "+-----+-----------+------------+\n",
            "| John|Engineering|   Project C|\n",
            "| John|Engineering|   Project A|\n",
            "|Alice|  Marketing|        NULL|\n",
            "|  Bob|Engineering|   Project B|\n",
            "|David|Engineering|   Project D|\n",
            "|Carol|      Sales|        NULL|\n",
            "+-----+-----------+------------+\n",
            "\n",
            "=== RIGHT JOIN ===\n",
            "+-----+-----------+------------+\n",
            "| name| department|project_name|\n",
            "+-----+-----------+------------+\n",
            "| John|Engineering|   Project A|\n",
            "|  Bob|Engineering|   Project B|\n",
            "| John|Engineering|   Project C|\n",
            "| NULL|       NULL|   Project E|\n",
            "|David|Engineering|   Project D|\n",
            "+-----+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Window Functions (Advanced Interview Topic)"
      ],
      "metadata": {
        "id": "uIn3sBwACHO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Salary ranking within departments\n",
        "dept_window = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
        "\n",
        "df_with_rank = df.withColumn(\"salary_rank\",\n",
        "                           row_number().over(dept_window)) \\\n",
        "                .withColumn(\"salary_dense_rank\",\n",
        "                           dense_rank().over(dept_window)) \\\n",
        "                .withColumn(\"department_avg_salary\",\n",
        "                           avg(\"salary\").over(Window.partitionBy(\"department\")))\n",
        "\n",
        "df_with_rank.show()\n",
        "\n",
        "# Running totals (common interview question)\n",
        "running_window = Window.orderBy(\"hire_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "df_running = df.withColumn(\"running_total_salary\",\n",
        "                          sum(\"salary\").over(running_window))\n",
        "df_running.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO3YFb9dB8oY",
        "outputId": "3f8b04c1-be0b-4263-fb07-5a62106b23bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-----------+------+----------+-----------+-----------------+---------------------+\n",
            "| id| name| department|salary| hire_date|salary_rank|salary_dense_rank|department_avg_salary|\n",
            "+---+-----+-----------+------+----------+-----------+-----------------+---------------------+\n",
            "|  5|David|Engineering| 85000|2018-08-12|          1|                1|              80000.0|\n",
            "|  3|  Bob|Engineering| 80000|2021-06-10|          2|                2|              80000.0|\n",
            "|  1| John|Engineering| 75000|2020-01-15|          3|                3|              80000.0|\n",
            "|  2|Alice|  Marketing| 65000|2019-03-20|          1|                1|              65000.0|\n",
            "|  4|Carol|      Sales| 70000|2020-11-05|          1|                1|              70000.0|\n",
            "+---+-----+-----------+------+----------+-----------+-----------------+---------------------+\n",
            "\n",
            "+---+-----+-----------+------+----------+--------------------+\n",
            "| id| name| department|salary| hire_date|running_total_salary|\n",
            "+---+-----+-----------+------+----------+--------------------+\n",
            "|  5|David|Engineering| 85000|2018-08-12|               85000|\n",
            "|  2|Alice|  Marketing| 65000|2019-03-20|              150000|\n",
            "|  1| John|Engineering| 75000|2020-01-15|              225000|\n",
            "|  4|Carol|      Sales| 70000|2020-11-05|              295000|\n",
            "|  3|  Bob|Engineering| 80000|2021-06-10|              375000|\n",
            "+---+-----+-----------+------+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Aggregations & GroupBy Operations"
      ],
      "metadata": {
        "id": "K7IE2ROlCXMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Department-wise statistics\n",
        "dept_stats = df.groupBy(\"department\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"employee_count\"),\n",
        "        avg(\"salary\").alias(\"avg_salary\"),\n",
        "        min(\"salary\").alias(\"min_salary\"),\n",
        "        max(\"salary\").alias(\"max_salary\"),\n",
        "        sum(\"salary\").alias(\"total_salary\")\n",
        "    ) \\\n",
        "    .orderBy(desc(\"avg_salary\"))\n",
        "\n",
        "dept_stats.show()\n",
        "\n",
        "# Multiple grouping levels\n",
        "from pyspark.sql.functions import year, month, to_date\n",
        "\n",
        "# Convert hire_date to proper date type\n",
        "df_with_dates = df.withColumn(\"hire_date\", to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "yearly_hiring = df_with_dates.groupBy(year(\"hire_date\").alias(\"year\")) \\\n",
        "    .agg(count(\"*\").alias(\"hires_count\")) \\\n",
        "    .orderBy(\"year\")\n",
        "\n",
        "yearly_hiring.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVuX40DcCKsh",
        "outputId": "d954d07e-c9fe-414e-92ec-b4cb72dbe761"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+----------+----------+----------+------------+\n",
            "| department|employee_count|avg_salary|min_salary|max_salary|total_salary|\n",
            "+-----------+--------------+----------+----------+----------+------------+\n",
            "|Engineering|             3|   80000.0|     75000|     85000|      240000|\n",
            "|      Sales|             1|   70000.0|     70000|     70000|       70000|\n",
            "|  Marketing|             1|   65000.0|     65000|     65000|       65000|\n",
            "+-----------+--------------+----------+----------+----------+------------+\n",
            "\n",
            "+----+-----------+\n",
            "|year|hires_count|\n",
            "+----+-----------+\n",
            "|2018|          1|\n",
            "|2019|          1|\n",
            "|2020|          2|\n",
            "|2021|          1|\n",
            "+----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chsoJHkoCc9h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}