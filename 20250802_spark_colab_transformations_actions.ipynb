{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1DYuaksAp+qi2YKXLg4n3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcims123/spark_in_colab/blob/main/20250802_spark_colab_transformations_actions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Technical Interview Preparation Guide for Google Colab (2025)\n",
        "## 🚀 Quick Google Colab Setup for PySpark\n",
        "### 1. Complete PySpark Installation (Latest Version 3.5+)"
      ],
      "metadata": {
        "id": "vkEsagQSUCzp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9eH8e_bTz9k",
        "outputId": "7ca30b28-9e54-4db8-f8a1-0582a29a640d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "✅ PySpark 3.5.0 installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# ===== CORRECTED PYSPARK SETUP FOR GOOGLE COLAB =====\n",
        "\n",
        "# Step 1: Install Java (required for Spark)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Step 2: Download Apache Spark 3.5.0 (stable version that works reliably)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Step 3: Install findspark\n",
        "!pip install findspark\n",
        "\n",
        "# Step 4: Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "# Step 5: Initialize Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "print(\"✅ PySpark 3.5.0 installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create Optimized Spark Session"
      ],
      "metadata": {
        "id": "bqx2DobeUWO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Create optimized Spark session for interviews\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"InterviewPrep\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"✅ Spark {spark.version} session created!\")\n",
        "print(f\"🔧 Using {spark.sparkContext.defaultParallelism} cores\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAnOLWFDUW5L",
        "outputId": "af46d2fc-191a-4d36-9698-1bd620cf3b85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Spark 3.5.0 session created!\n",
            "🔧 Using 2 cores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete PySpark Transformations vs Actions Guide\n",
        "\n",
        "## 🔍 Core Concept\n",
        "\n",
        "**TRANSFORMATIONS** are **LAZY** - they build a computation graph (DAG) but don't execute until an **ACTION** is called.\n",
        "**ACTIONS** are **EAGER** - they trigger the execution of all transformations in the lineage and return results.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 All PySpark Transformations (Lazy Operations)\n",
        "\n",
        "### Basic Transformations Table\n",
        "\n",
        "| Transformation | Purpose | Returns | Example |\n",
        "|----------------|---------|---------|---------|\n",
        "| `select()` | Choose specific columns | DataFrame | `df.select(\"name\", \"salary\")` |\n",
        "| `filter()` / `where()` | Filter rows by condition | DataFrame | `df.filter(col(\"age\") > 25)` |\n",
        "| `withColumn()` | Add/modify a column | DataFrame | `df.withColumn(\"bonus\", col(\"salary\") * 0.1)` |\n",
        "| `withColumnRenamed()` | Rename a column | DataFrame | `df.withColumnRenamed(\"old_name\", \"new_name\")` |\n",
        "| `drop()` | Remove columns | DataFrame | `df.drop(\"unwanted_col\")` |\n",
        "| `distinct()` | Remove duplicate rows | DataFrame | `df.distinct()` |\n",
        "| `dropDuplicates()` | Remove duplicates by columns | DataFrame | `df.dropDuplicates([\"name\", \"dept\"])` |\n",
        "\n",
        "### Aggregation Transformations Table\n",
        "\n",
        "| Transformation | Purpose | Returns | Example |\n",
        "|----------------|---------|---------|---------|\n",
        "| `groupBy()` | Group rows by columns | GroupedData | `df.groupBy(\"department\")` |\n",
        "| `agg()` | Apply aggregation functions | DataFrame | `df.groupBy(\"dept\").agg(avg(\"salary\"))` |\n",
        "| `orderBy()` / `sort()` | Sort DataFrame | DataFrame | `df.orderBy(\"salary\")` |\n",
        "| `limit()` | Limit number of rows | DataFrame | `df.limit(10)` |\n",
        "\n",
        "### Join & Union Transformations Table\n",
        "\n",
        "| Transformation | Purpose | Returns | Example |\n",
        "|----------------|---------|---------|---------|\n",
        "| `join()` | Join two DataFrames | DataFrame | `df1.join(df2, \"id\")` |\n",
        "| `union()` | Union two DataFrames | DataFrame | `df1.union(df2)` |\n",
        "| `unionByName()` | Union by column names | DataFrame | `df1.unionByName(df2)` |\n",
        "| `intersect()` | Find common rows | DataFrame | `df1.intersect(df2)` |\n",
        "| `except()` / `subtract()` | Subtract rows | DataFrame | `df1.except(df2)` |\n",
        "\n",
        "### Data Manipulation Transformations Table\n",
        "\n",
        "| Transformation | Purpose | Returns | Example |\n",
        "|----------------|---------|---------|---------|\n",
        "| `cast()` | Change column data type | Column | `col(\"age\").cast(\"string\")` |\n",
        "| `alias()` | Give column an alias | Column | `col(\"salary\").alias(\"pay\")` |\n",
        "| `when()` / `otherwise()` | Conditional logic | Column | `when(col(\"age\") > 30, \"Senior\")` |\n",
        "| `coalesce()` | Reduce number of partitions | DataFrame | `df.coalesce(2)` |\n",
        "| `repartition()` | Change number of partitions | DataFrame | `df.repartition(4)` |\n",
        "| `sample()` | Sample fraction of data | DataFrame | `df.sample(0.1)` |\n",
        "\n",
        "### Advanced Transformations Table\n",
        "\n",
        "| Transformation | Purpose | Returns | Example |\n",
        "|----------------|---------|---------|---------|\n",
        "| `explode()` | Explode array/map column | Column | `df.select(explode(\"array_col\"))` |\n",
        "| `pivot()` | Pivot table transformation | DataFrame | `df.groupBy(\"A\").pivot(\"B\").sum(\"C\")` |\n",
        "| `unpivot()` | Unpivot transformation | DataFrame | `df.unpivot([\"A\"], [\"B\", \"C\"])` |\n",
        "| `withColumn()` + window | Window functions | DataFrame | `df.withColumn(\"rank\", row_number().over(window))` |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ All PySpark Actions (Eager Operations)\n",
        "\n",
        "### Data Collection Actions Table\n",
        "\n",
        "| Action | Purpose | Returns | Performance Impact |\n",
        "|--------|---------|---------|-------------------|\n",
        "| `show()` | Display DataFrame rows | None | Low (only fetches displayed rows) |\n",
        "| `collect()` | Bring all data to driver | List[Row] | **HIGH** - Avoid for large data |\n",
        "| `take(n)` | Take first n rows | List[Row] | Low (only n rows) |\n",
        "| `head(n)` | Same as take(n) | List[Row] | Low (only n rows) |\n",
        "| `first()` | Get first row | Row | Low (only 1 row) |\n",
        "| `tail(n)` | Get last n rows | List[Row] | Medium (requires sorting) |\n",
        "\n",
        "### Counting & Statistical Actions Table\n",
        "\n",
        "| Action | Purpose | Returns | Performance Impact |\n",
        "|--------|---------|---------|-------------------|\n",
        "| `count()` | Count total rows | int | Medium (scans all data) |\n",
        "| `describe()` | Statistical summary | DataFrame | Medium (computes stats) |\n",
        "| `summary()` | Extended statistics | DataFrame | Medium (computes stats) |\n",
        "| `min()` | Minimum value of column | Row | Medium (scans column) |\n",
        "| `max()` | Maximum value of column | Row | Medium (scans column) |\n",
        "\n",
        "### Data Output Actions Table\n",
        "\n",
        "| Action | Purpose | Returns | Performance Impact |\n",
        "|--------|---------|---------|-------------------|\n",
        "| `write()` | Write to storage | DataFrameWriter | Medium-High (depends on size) |\n",
        "| `save()` | Save DataFrame | None | Medium-High (depends on size) |\n",
        "| `saveAsTable()` | Save as Hive table | None | Medium-High (depends on size) |\n",
        "\n",
        "### Advanced Actions Table\n",
        "\n",
        "| Action | Purpose | Returns | Performance Impact |\n",
        "|--------|---------|---------|-------------------|\n",
        "| `foreach()` | Apply function to each row | None | High (processes all rows) |\n",
        "| `foreachPartition()` | Apply function to each partition | None | Medium-High (partition-wise) |\n",
        "| `reduce()` | Reduce rows to single value | Any | High (processes all data) |\n",
        "\n",
        "---\n",
        "\n",
        "## 💻 Complete Code Examples"
      ],
      "metadata": {
        "id": "ljVlA5jVUeHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# COMPLETE PYSPARK TRANSFORMATIONS VS ACTIONS\n",
        "# ===============================================\n",
        "\n",
        "# Setup\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import time\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"TransformationsVsActions\").getOrCreate()\n",
        "\n",
        "# Sample employee data\n",
        "employees_data = [\n",
        "    (1, \"John\", \"Engineering\", 75000, \"2020-01-15\", [\"Python\", \"Spark\"]),\n",
        "    (2, \"Alice\", \"Marketing\", 65000, \"2019-03-20\", [\"SQL\", \"Tableau\"]),\n",
        "    (3, \"Bob\", \"Engineering\", 80000, \"2021-06-10\", [\"Java\", \"Scala\"]),\n",
        "    (4, \"Carol\", \"Sales\", 70000, \"2020-11-05\", [\"Excel\", \"PowerBI\"]),\n",
        "    (5, \"David\", \"Engineering\", 85000, \"2018-08-12\", [\"Python\", \"Java\"]),\n",
        "    (6, \"Eve\", \"Marketing\", 68000, \"2021-02-28\", [\"SQL\", \"Python\"]),\n",
        "    (7, \"Frank\", \"Sales\", 72000, \"2019-12-15\", [\"Salesforce\"])\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"department\", StringType(), True),\n",
        "    StructField(\"salary\", IntegerType(), True),\n",
        "    StructField(\"hire_date\", StringType(), True),\n",
        "    StructField(\"skills\", ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(employees_data, schema)\n",
        "print(\"=== Original DataFrame ===\")\n",
        "df.show()\n",
        "\n",
        "# ================================================\n",
        "# TRANSFORMATIONS (LAZY OPERATIONS) - BUILD GRAPH\n",
        "# ================================================\n",
        "\n",
        "print(\"\\n🔄 ALL TRANSFORMATIONS (LAZY - NO EXECUTION)\")\n",
        "\n",
        "# 1. BASIC TRANSFORMATIONS\n",
        "print(\"--- Basic Transformations ---\")\n",
        "select_trans = df.select(\"name\", \"department\", \"salary\")\n",
        "filter_trans = df.filter(col(\"salary\") > 70000)\n",
        "where_trans = df.where(col(\"department\") == \"Engineering\")\n",
        "bonus_trans = df.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
        "category_trans = df.withColumn(\"salary_category\",\n",
        "                              when(col(\"salary\") > 75000, \"High\")\n",
        "                              .when(col(\"salary\") > 65000, \"Medium\")\n",
        "                              .otherwise(\"Low\"))\n",
        "renamed_trans = df.withColumnRenamed(\"department\", \"dept\")\n",
        "dropped_trans = df.drop(\"hire_date\")\n",
        "distinct_trans = df.select(\"department\").distinct()\n",
        "print(\"✅ select(), filter(), where(), withColumn(), withColumnRenamed(), drop(), distinct() - ALL LAZY\")\n",
        "\n",
        "# 2. AGGREGATION TRANSFORMATIONS\n",
        "print(\"\\n--- Aggregation Transformations ---\")\n",
        "dept_stats = df.groupBy(\"department\").agg(\n",
        "    avg(\"salary\").alias(\"avg_salary\"),\n",
        "    count(\"*\").alias(\"employee_count\"),\n",
        "    min(\"salary\").alias(\"min_salary\"),\n",
        "    max(\"salary\").alias(\"max_salary\")\n",
        ")\n",
        "sorted_trans = df.orderBy(desc(\"salary\"))\n",
        "sort_trans = df.sort(\"name\")\n",
        "limited_trans = df.limit(3)\n",
        "print(\"✅ groupBy(), agg(), orderBy(), sort(), limit() - ALL LAZY\")\n",
        "\n",
        "# 3. JOIN & UNION TRANSFORMATIONS\n",
        "print(\"\\n--- Join & Union Transformations ---\")\n",
        "projects_data = [(101, \"Project Alpha\", 1), (102, \"Project Beta\", 3), (103, \"Project Gamma\", 5)]\n",
        "projects_df = spark.createDataFrame(projects_data, [\"project_id\", \"project_name\", \"lead_id\"])\n",
        "\n",
        "inner_join_trans = df.join(projects_df, df.id == projects_df.lead_id, \"inner\")\n",
        "left_join_trans = df.join(projects_df, df.id == projects_df.lead_id, \"left\")\n",
        "broadcast_join_trans = df.join(broadcast(projects_df), df.id == projects_df.lead_id, \"inner\")\n",
        "\n",
        "new_employees = spark.createDataFrame([(8, \"Grace\", \"HR\", 60000, \"2022-01-10\", [\"Excel\"])], schema)\n",
        "union_trans = df.union(new_employees)\n",
        "intersect_trans = df.select(\"department\").intersect(projects_df.select(lit(\"Engineering\").alias(\"department\")))\n",
        "except_trans = df.select(\"name\").exceptAll(new_employees.select(\"name\"))\n",
        "print(\"✅ join(), union(), intersect(), exceptAll(), broadcast() - ALL LAZY\")\n",
        "\n",
        "# 4. ADVANCED TRANSFORMATIONS\n",
        "print(\"\\n--- Advanced Transformations ---\")\n",
        "exploded_skills = df.select(\"name\", explode(\"skills\").alias(\"skill\"))\n",
        "\n",
        "window_spec = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
        "windowed_trans = df.withColumn(\"dept_rank\", row_number().over(window_spec))\n",
        "\n",
        "pivot_trans = df.groupBy(\"department\").pivot(\"name\").agg(first(\"salary\"))\n",
        "repartitioned_trans = df.repartition(4, \"department\")\n",
        "coalesced_trans = df.coalesce(2)\n",
        "sampled_trans = df.sample(0.5, seed=42)\n",
        "\n",
        "# Date transformations\n",
        "date_trans = df.withColumn(\"hire_year\", year(to_date(\"hire_date\", \"yyyy-MM-dd\")))\n",
        "\n",
        "# Null handling transformations\n",
        "null_filled_trans = df.fillna({\"salary\": 0, \"department\": \"Unknown\"})\n",
        "null_dropped_trans = df.dropna(subset=[\"name\"])\n",
        "print(\"✅ explode(), window functions, pivot(), repartition(), coalesce(), sample(), date functions, null handling - ALL LAZY\")\n",
        "\n",
        "print(\"\\n🚨 IMPORTANT: ALL ABOVE OPERATIONS ARE LAZY - NO EXECUTION YET!\")\n",
        "print(\"The computation graph (DAG) is built but not executed.\")\n",
        "\n",
        "# ============================================\n",
        "# ACTIONS (EAGER OPERATIONS) - TRIGGER EXECUTION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\n⚡ ALL ACTIONS (EAGER - TRIGGER EXECUTION)\")\n",
        "\n",
        "# 1. DATA COLLECTION ACTIONS\n",
        "print(\"\\n--- Data Collection Actions ---\")\n",
        "print(\"=== SHOW - Display sample data ===\")\n",
        "select_trans.show(3)  # Shows 3 rows\n",
        "\n",
        "print(\"\\n=== TAKE/HEAD - Get first n rows ===\")\n",
        "first_3 = df.take(3)\n",
        "head_2 = df.head(2)\n",
        "print(f\"Take(3) returned {len(first_3)} rows\")\n",
        "print(f\"Head(2) returned {len(head_2)} rows\")\n",
        "\n",
        "print(\"\\n=== FIRST - Get first row ===\")\n",
        "first_row = df.first()\n",
        "print(f\"First employee: {first_row['name']}\")\n",
        "\n",
        "print(\"\\n=== COLLECT - Bring all data to driver (CAREFUL!) ===\")\n",
        "small_collected = df.select(\"name\", \"department\").collect()\n",
        "print(f\"Collected {len(small_collected)} rows - USE ONLY FOR SMALL DATA!\")\n",
        "\n",
        "# 2. COUNTING & STATISTICAL ACTIONS\n",
        "print(\"\\n--- Counting & Statistical Actions ---\")\n",
        "print(\"=== COUNT - Total rows ===\")\n",
        "total_rows = df.count()\n",
        "print(f\"Total employees: {total_rows}\")\n",
        "\n",
        "print(\"\\n=== DESCRIBE - Statistical summary ===\")\n",
        "df.describe(\"salary\").show()\n",
        "\n",
        "print(\"\\n=== MIN/MAX - Column extremes ===\")\n",
        "min_salary = df.agg(min(\"salary\")).collect()[0][0]\n",
        "max_salary = df.agg(max(\"salary\")).collect()[0][0]\n",
        "print(f\"Salary range: ${min_salary:,} - ${max_salary:,}\")\n",
        "\n",
        "# 3. ADVANCED ACTIONS\n",
        "print(\"\\n--- Advanced Actions ---\")\n",
        "print(\"=== FOREACH - Process each row ===\")\n",
        "def print_high_earner(row):\n",
        "    if row['salary'] > 75000:\n",
        "        print(f\"High earner: {row['name']} - ${row['salary']:,}\")\n",
        "\n",
        "high_earners = df.filter(col(\"salary\") > 75000)\n",
        "high_earners.foreach(print_high_earner)\n",
        "\n",
        "print(\"\\n=== REDUCE - Aggregate to single value ===\")\n",
        "total_salary = df.rdd.map(lambda row: row['salary']).reduce(lambda a, b: a + b)\n",
        "print(f\"Total company salary: ${total_salary:,}\")\n",
        "\n",
        "# 4. OUTPUT ACTIONS\n",
        "print(\"\\n--- Output Actions ---\")\n",
        "print(\"=== WRITE - Save to storage ===\")\n",
        "# df.write.mode(\"overwrite\").parquet(\"output/employees\")  # Uncomment to actually write\n",
        "print(\"✅ write.parquet() - saves DataFrame to storage\")\n",
        "\n",
        "print(\"\\n=== CACHE/PERSIST - Store in memory for reuse ===\")\n",
        "cached_df = df.cache()  # Marks for caching\n",
        "cached_count = cached_df.count()  # Triggers caching\n",
        "print(f\"DataFrame cached with {cached_count} rows\")\n",
        "\n",
        "# ============================================\n",
        "# PERFORMANCE DEMONSTRATION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\n📈 PERFORMANCE COMPARISON DEMO\")\n",
        "\n",
        "# Multiple transformations (all lazy)\n",
        "start_time = time.time()\n",
        "complex_transformation = df.filter(col(\"salary\") > 60000) \\\n",
        "                          .withColumn(\"bonus\", col(\"salary\") * 0.15) \\\n",
        "                          .withColumn(\"total_comp\", col(\"salary\") + col(\"bonus\")) \\\n",
        "                          .groupBy(\"department\") \\\n",
        "                          .agg(avg(\"total_comp\").alias(\"avg_total_comp\")) \\\n",
        "                          .orderBy(desc(\"avg_total_comp\"))\n",
        "transform_time = time.time() - start_time\n",
        "print(f\"⏱️ Creating complex transformation chain: {transform_time:.6f} seconds (LAZY)\")\n",
        "\n",
        "# Trigger execution with action\n",
        "start_time = time.time()\n",
        "complex_transformation.show()\n",
        "action_time = time.time() - start_time\n",
        "print(f\"⏱️ Executing with show() action: {action_time:.6f} seconds (EAGER)\")\n",
        "\n",
        "# ============================================\n",
        "# CACHING PERFORMANCE DEMO\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n📊 CACHING PERFORMANCE DEMO\")\n",
        "\n",
        "expensive_transformation = df.filter(col(\"salary\") > 70000) \\\n",
        "                             .groupBy(\"department\") \\\n",
        "                             .agg(avg(\"salary\").alias(\"avg_sal\"), count(\"*\").alias(\"count\"))\n",
        "\n",
        "# Without caching - multiple actions recompute\n",
        "start_time = time.time()\n",
        "count1 = expensive_transformation.count()\n",
        "result1 = expensive_transformation.collect()\n",
        "no_cache_time = time.time() - start_time\n",
        "print(f\"⏱️ Two actions WITHOUT caching: {no_cache_time:.6f} seconds\")\n",
        "\n",
        "# With caching - computation reused\n",
        "cached_transformation = expensive_transformation.cache()\n",
        "start_time = time.time()\n",
        "count2 = cached_transformation.count()  # Computes and caches\n",
        "result2 = cached_transformation.collect()  # Uses cache\n",
        "cache_time = time.time() - start_time\n",
        "print(f\"⏱️ Two actions WITH caching: {cache_time:.6f} seconds\")\n",
        "print(f\"🚀 Caching speedup: {no_cache_time/cache_time:.1f}x faster\")\n",
        "\n",
        "# ============================================\n",
        "# COMMON INTERVIEW SCENARIOS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\n🎯 COMMON INTERVIEW SCENARIOS\")\n",
        "\n",
        "print(\"\\n--- Scenario 1: Chain multiple transformations ---\")\n",
        "# This is efficient - all transformations are lazy until action\n",
        "chained_result = df.select(\"name\", \"department\", \"salary\") \\\n",
        "                   .filter(col(\"salary\") > 65000) \\\n",
        "                   .withColumn(\"bonus\", col(\"salary\") * 0.1) \\\n",
        "                   .groupBy(\"department\") \\\n",
        "                   .agg(avg(\"bonus\").alias(\"avg_bonus\")) \\\n",
        "                   .orderBy(desc(\"avg_bonus\"))\n",
        "\n",
        "print(\"✅ Chained 5 transformations efficiently (lazy)\")\n",
        "chained_result.show()  # Single action executes entire chain\n",
        "\n",
        "print(\"\\n--- Scenario 2: Window functions with ranking ---\")\n",
        "window_by_dept = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
        "ranked_employees = df.withColumn(\"rank\", row_number().over(window_by_dept)) \\\n",
        "                     .withColumn(\"dense_rank\", dense_rank().over(window_by_dept))\n",
        "print(\"✅ Window function transformations created (lazy)\")\n",
        "ranked_employees.show()  # Action triggers execution\n",
        "\n",
        "print(\"\\n--- Scenario 3: Complex joins with aggregations ---\")\n",
        "dept_project_stats = df.join(projects_df, df.id == projects_df.lead_id, \"left\") \\\n",
        "                        .groupBy(\"department\") \\\n",
        "                        .agg(count(\"project_id\").alias(\"project_count\"),\n",
        "                             countDistinct(\"project_id\").alias(\"unique_projects\"),\n",
        "                             avg(\"salary\").alias(\"avg_salary\"))\n",
        "print(\"✅ Join + aggregation transformation created (lazy)\")\n",
        "dept_project_stats.show()  # Action executes join and aggregation\n",
        "\n",
        "# ============================================\n",
        "# COMMON MISTAKES TO AVOID\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\\n❌ COMMON MISTAKES TO AVOID\")\n",
        "\n",
        "print(\"\\n--- Mistake 1: Multiple actions without caching ---\")\n",
        "print(\"❌ BAD: df.filter(...).count() + df.filter(...).show() - recomputes filter twice\")\n",
        "print(\"✅ GOOD: cached = df.filter(...).cache(); cached.count(); cached.show()\")\n",
        "\n",
        "print(\"\\n--- Mistake 2: Using collect() on large data ---\")\n",
        "print(\"❌ BAD: large_df.collect() - brings all data to driver (OOM risk)\")\n",
        "print(\"✅ GOOD: large_df.show(20) or large_df.take(100)\")\n",
        "\n",
        "print(\"\\n--- Mistake 3: Not understanding lazy evaluation ---\")\n",
        "print(\"❌ BAD: Expecting transformations to execute immediately\")\n",
        "print(\"✅ GOOD: Chain transformations, then trigger with action\")\n",
        "\n",
        "print(\"\\n--- Mistake 4: Inefficient joins ---\")\n",
        "print(\"❌ BAD: large_df.join(huge_df) - both large datasets\")\n",
        "print(\"✅ GOOD: large_df.join(broadcast(small_df)) - broadcast small dataset\")\n",
        "\n",
        "print(\"\\n\\n✅ COMPLETE TRANSFORMATIONS vs ACTIONS DEMO FINISHED!\")\n",
        "print(\"🎯 You now understand the full spectrum of PySpark lazy vs eager operations!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUZyiiaFYbd6",
        "outputId": "37f1921a-60d3-4be7-9986-c27ad5f4b9e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Original DataFrame ===\n",
            "+---+-----+-----------+------+----------+----------------+\n",
            "| id| name| department|salary| hire_date|          skills|\n",
            "+---+-----+-----------+------+----------+----------------+\n",
            "|  1| John|Engineering| 75000|2020-01-15| [Python, Spark]|\n",
            "|  2|Alice|  Marketing| 65000|2019-03-20|  [SQL, Tableau]|\n",
            "|  3|  Bob|Engineering| 80000|2021-06-10|   [Java, Scala]|\n",
            "|  4|Carol|      Sales| 70000|2020-11-05|[Excel, PowerBI]|\n",
            "|  5|David|Engineering| 85000|2018-08-12|  [Python, Java]|\n",
            "|  6|  Eve|  Marketing| 68000|2021-02-28|   [SQL, Python]|\n",
            "|  7|Frank|      Sales| 72000|2019-12-15|    [Salesforce]|\n",
            "+---+-----+-----------+------+----------+----------------+\n",
            "\n",
            "\n",
            "🔄 ALL TRANSFORMATIONS (LAZY - NO EXECUTION)\n",
            "--- Basic Transformations ---\n",
            "✅ select(), filter(), where(), withColumn(), withColumnRenamed(), drop(), distinct() - ALL LAZY\n",
            "\n",
            "--- Aggregation Transformations ---\n",
            "✅ groupBy(), agg(), orderBy(), sort(), limit() - ALL LAZY\n",
            "\n",
            "--- Join & Union Transformations ---\n",
            "✅ join(), union(), intersect(), exceptAll(), broadcast() - ALL LAZY\n",
            "\n",
            "--- Advanced Transformations ---\n",
            "✅ explode(), window functions, pivot(), repartition(), coalesce(), sample(), date functions, null handling - ALL LAZY\n",
            "\n",
            "🚨 IMPORTANT: ALL ABOVE OPERATIONS ARE LAZY - NO EXECUTION YET!\n",
            "The computation graph (DAG) is built but not executed.\n",
            "\n",
            "\n",
            "⚡ ALL ACTIONS (EAGER - TRIGGER EXECUTION)\n",
            "\n",
            "--- Data Collection Actions ---\n",
            "=== SHOW - Display sample data ===\n",
            "+-----+-----------+------+\n",
            "| name| department|salary|\n",
            "+-----+-----------+------+\n",
            "| John|Engineering| 75000|\n",
            "|Alice|  Marketing| 65000|\n",
            "|  Bob|Engineering| 80000|\n",
            "+-----+-----------+------+\n",
            "only showing top 3 rows\n",
            "\n",
            "\n",
            "=== TAKE/HEAD - Get first n rows ===\n",
            "Take(3) returned 3 rows\n",
            "Head(2) returned 2 rows\n",
            "\n",
            "=== FIRST - Get first row ===\n",
            "First employee: John\n",
            "\n",
            "=== COLLECT - Bring all data to driver (CAREFUL!) ===\n",
            "Collected 7 rows - USE ONLY FOR SMALL DATA!\n",
            "\n",
            "--- Counting & Statistical Actions ---\n",
            "=== COUNT - Total rows ===\n",
            "Total employees: 7\n",
            "\n",
            "=== DESCRIBE - Statistical summary ===\n",
            "+-------+-----------------+\n",
            "|summary|           salary|\n",
            "+-------+-----------------+\n",
            "|  count|                7|\n",
            "|   mean|73571.42857142857|\n",
            "| stddev|6996.597812678741|\n",
            "|    min|            65000|\n",
            "|    max|            85000|\n",
            "+-------+-----------------+\n",
            "\n",
            "\n",
            "=== MIN/MAX - Column extremes ===\n",
            "Salary range: $65,000 - $85,000\n",
            "\n",
            "--- Advanced Actions ---\n",
            "=== FOREACH - Process each row ===\n",
            "\n",
            "=== REDUCE - Aggregate to single value ===\n",
            "Total company salary: $515,000\n",
            "\n",
            "--- Output Actions ---\n",
            "=== WRITE - Save to storage ===\n",
            "✅ write.parquet() - saves DataFrame to storage\n",
            "\n",
            "=== CACHE/PERSIST - Store in memory for reuse ===\n",
            "DataFrame cached with 7 rows\n",
            "\n",
            "\n",
            "📈 PERFORMANCE COMPARISON DEMO\n",
            "⏱️ Creating complex transformation chain: 0.080667 seconds (LAZY)\n",
            "+-----------+--------------+\n",
            "| department|avg_total_comp|\n",
            "+-----------+--------------+\n",
            "|Engineering|       92000.0|\n",
            "|      Sales|       81650.0|\n",
            "|  Marketing|       76475.0|\n",
            "+-----------+--------------+\n",
            "\n",
            "⏱️ Executing with show() action: 0.659389 seconds (EAGER)\n",
            "\n",
            "📊 CACHING PERFORMANCE DEMO\n",
            "⏱️ Two actions WITHOUT caching: 0.837331 seconds\n",
            "⏱️ Two actions WITH caching: 0.628521 seconds\n",
            "🚀 Caching speedup: 1.3x faster\n",
            "\n",
            "\n",
            "🎯 COMMON INTERVIEW SCENARIOS\n",
            "\n",
            "--- Scenario 1: Chain multiple transformations ---\n",
            "✅ Chained 5 transformations efficiently (lazy)\n",
            "+-----------+---------+\n",
            "| department|avg_bonus|\n",
            "+-----------+---------+\n",
            "|Engineering|   8000.0|\n",
            "|      Sales|   7100.0|\n",
            "|  Marketing|   6800.0|\n",
            "+-----------+---------+\n",
            "\n",
            "\n",
            "--- Scenario 2: Window functions with ranking ---\n",
            "✅ Window function transformations created (lazy)\n",
            "+---+-----+-----------+------+----------+----------------+----+----------+\n",
            "| id| name| department|salary| hire_date|          skills|rank|dense_rank|\n",
            "+---+-----+-----------+------+----------+----------------+----+----------+\n",
            "|  5|David|Engineering| 85000|2018-08-12|  [Python, Java]|   1|         1|\n",
            "|  3|  Bob|Engineering| 80000|2021-06-10|   [Java, Scala]|   2|         2|\n",
            "|  1| John|Engineering| 75000|2020-01-15| [Python, Spark]|   3|         3|\n",
            "|  6|  Eve|  Marketing| 68000|2021-02-28|   [SQL, Python]|   1|         1|\n",
            "|  2|Alice|  Marketing| 65000|2019-03-20|  [SQL, Tableau]|   2|         2|\n",
            "|  7|Frank|      Sales| 72000|2019-12-15|    [Salesforce]|   1|         1|\n",
            "|  4|Carol|      Sales| 70000|2020-11-05|[Excel, PowerBI]|   2|         2|\n",
            "+---+-----+-----------+------+----------+----------------+----+----------+\n",
            "\n",
            "\n",
            "--- Scenario 3: Complex joins with aggregations ---\n",
            "✅ Join + aggregation transformation created (lazy)\n",
            "+-----------+-------------+---------------+----------+\n",
            "| department|project_count|unique_projects|avg_salary|\n",
            "+-----------+-------------+---------------+----------+\n",
            "|      Sales|            0|              0|   71000.0|\n",
            "|Engineering|            3|              3|   80000.0|\n",
            "|  Marketing|            0|              0|   66500.0|\n",
            "+-----------+-------------+---------------+----------+\n",
            "\n",
            "\n",
            "\n",
            "❌ COMMON MISTAKES TO AVOID\n",
            "\n",
            "--- Mistake 1: Multiple actions without caching ---\n",
            "❌ BAD: df.filter(...).count() + df.filter(...).show() - recomputes filter twice\n",
            "✅ GOOD: cached = df.filter(...).cache(); cached.count(); cached.show()\n",
            "\n",
            "--- Mistake 2: Using collect() on large data ---\n",
            "❌ BAD: large_df.collect() - brings all data to driver (OOM risk)\n",
            "✅ GOOD: large_df.show(20) or large_df.take(100)\n",
            "\n",
            "--- Mistake 3: Not understanding lazy evaluation ---\n",
            "❌ BAD: Expecting transformations to execute immediately\n",
            "✅ GOOD: Chain transformations, then trigger with action\n",
            "\n",
            "--- Mistake 4: Inefficient joins ---\n",
            "❌ BAD: large_df.join(huge_df) - both large datasets\n",
            "✅ GOOD: large_df.join(broadcast(small_df)) - broadcast small dataset\n",
            "\n",
            "\n",
            "✅ COMPLETE TRANSFORMATIONS vs ACTIONS DEMO FINISHED!\n",
            "🎯 You now understand the full spectrum of PySpark lazy vs eager operations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 🎯 Key Takeaways for Interviews\n",
        "\n",
        "### Must Remember:\n",
        "\n",
        "1. **Transformations are LAZY** - they build a computation graph\n",
        "2. **Actions are EAGER** - they trigger execution and return results\n",
        "3. **Common transformations**: `select()`, `filter()`, `withColumn()`, `groupBy()`, `join()`\n",
        "4. **Common actions**: `show()`, `count()`, `collect()`, `take()`, `write()`\n",
        "5. **Performance**: Cache DataFrames used multiple times\n",
        "6. **Memory**: Avoid `collect()` on large DataFrames\n",
        "\n",
        "### Interview Questions to Expect:\n",
        "\n",
        "- \"What's the difference between transformations and actions?\"\n",
        "- \"Why does Spark use lazy evaluation?\"\n",
        "- \"When would you use `cache()` or `persist()`?\"\n",
        "- \"What's wrong with using `collect()` on a large DataFrame?\"\n",
        "- \"How do you optimize a slow Spark job?\"\n",
        "\n",
        "### Performance Best Practices:\n",
        "\n",
        "#### ✅ DO:\n",
        "- Chain transformations before actions\n",
        "- Cache DataFrames used multiple times\n",
        "- Use `broadcast()` for small datasets in joins\n",
        "- Use `show()` for displaying data\n",
        "- Use `take(n)` for small samples\n",
        "\n",
        "#### ❌ DON'T:\n",
        "- Use `collect()` on large DataFrames\n",
        "- Create multiple actions without caching\n",
        "- Ignore data skew in joins\n",
        "- Use UDFs when built-in functions exist\n",
        "- Forget to unpersist cached DataFrames\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Final Interview Tips\n",
        "\n",
        "### Core Concepts to Articulate:\n",
        "\n",
        "1. **Lazy Evaluation Benefits:**\n",
        "   - Query optimization through Catalyst optimizer\n",
        "   - Fault tolerance through lineage tracking\n",
        "   - Resource efficiency - only compute what's needed\n",
        "   - Avoiding unnecessary intermediate results\n",
        "\n",
        "2. **When to Cache:**\n",
        "   - DataFrame is used multiple times\n",
        "   - Expensive computations (joins, aggregations)\n",
        "   - Iterative algorithms (machine learning)\n",
        "   - Breaking long lineage chains\n",
        "\n",
        "3. **Memory Management:**\n",
        "   - Understanding driver vs executor memory\n",
        "   - Avoiding driver OOM with `collect()`\n",
        "   - Proper partitioning strategies\n",
        "   - Using appropriate storage levels\n",
        "\n",
        "4. **Optimization Strategies:**\n",
        "   - Predicate pushdown\n",
        "   - Projection pruning\n",
        "   - Broadcast joins for small datasets\n",
        "   - Bucketing for frequent joins\n",
        "   - Proper data serialization\n",
        "\n",
        "This comprehensive guide covers all essential transformations and actions you'll need for PySpark technical interviews! 🎯"
      ],
      "metadata": {
        "id": "CFQqBfBuYfsu"
      }
    }
  ]
}